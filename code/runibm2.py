import nltk
import string
from preprocess import compile_corpus
from nltk.translate import IBMModel2, AlignedSent, Alignment
from nltk.metrics import precision, recall
from nltk.tokenize import TweetTokenizer

def run(filename, iterations):
    # global variables utilized in the assessment of the IBM Model
    global ibm2
    global corpus

    # construct and modify corpus by adding the system alignments to every sentence pair    
    corpus = compile_corpus(filename)
    ibm2 = IBMModel2(corpus, iterations)

    # produce the alignments of the test sentences
    get_alignments("data/evaluation tests/test sentences/test.spanish")

def get_alignments(filename):
    f = open(filename, "r")

    for line in f:
        # lowercase all words in the line -- this includes eng + for sentence
        line = line.lower()
        strs = line.split("\t")

        for_text = strs[0]
        eng_text = strs[1]
        ibm1_aligns = strs[2]
        # Tokenize on white space
        tokenizer = TweetTokenizer()
        eng_words = tokenizer.tokenize(eng_text)
        for_words = tokenizer.tokenize(for_text)
        
        for a in corpus:
            # only print out "valid" sentence pairs
            # valid = sentence pairs with system-created alignments
            try:
                # print out the sentences for the random sentences generated by IBM 1
                if(a.mots == eng_words and a.words == for_words):
                    print(" ".join(a.mots), "\t", " ".join(a.words), "\t", ibm1_aligns, "\t", a.alignment)
            except:
                pass
    f.close



def main():
    # change the file based on the langauge being tested
    run("data/languages/vie-eng.txt", 5)
    
if __name__ == "__main__":
    main()
